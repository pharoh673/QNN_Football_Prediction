{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13978217,"sourceType":"datasetVersion","datasetId":1731043},{"sourceId":14010293,"sourceType":"datasetVersion","datasetId":8924989},{"sourceId":283961162,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: basic setup â€“ install \n\n# On Kaggle \n!pip install -q pennylane pennylane-lightning torch torchvision\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pennylane as qml\nimport math\nimport random\n\n# Reproducibility helpers\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"PennyLane version:\", qml.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:50:02.459900Z","iopub.execute_input":"2025-12-05T10:50:02.460596Z","iopub.status.idle":"2025-12-05T10:51:28.959555Z","shell.execute_reply.started":"2025-12-05T10:50:02.460573Z","shell.execute_reply":"2025-12-05T10:51:28.958826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q \"pennylane\" \"pennylane-lightning[gpu]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:51:32.944498Z","iopub.execute_input":"2025-12-05T10:51:32.945322Z","iopub.status.idle":"2025-12-05T10:51:40.187561Z","shell.execute_reply.started":"2025-12-05T10:51:32.945293Z","shell.execute_reply":"2025-12-05T10:51:40.186628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2 configure Torch + PennyLane devices with GPU-first, CPU fallback\n\n# PyTorch device selection\ntorch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"PyTorch running on:\", torch_device)\n\n# Number of qubits for our QNN\nN_QUBITS = 4\n\n# Quantum device selection\ntry:\n    q_device = qml.device(\"lightning.gpu\", wires=N_QUBITS)\n    print(\"PennyLane quantum device: lightning.gpu (GPU)\")\nexcept Exception as e:\n    print(\"GPU quantum simulator not available â†’ using default.qubit (CPU).\")\n    print(\"Reason:\", repr(e))\n    q_device = qml.device(\"default.qubit\", wires=N_QUBITS)\n    print(\"PennyLane quantum device: default.qubit (CPU)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:52:49.580406Z","iopub.execute_input":"2025-12-05T10:52:49.581001Z","iopub.status.idle":"2025-12-05T10:52:49.831683Z","shell.execute_reply.started":"2025-12-05T10:52:49.580970Z","shell.execute_reply":"2025-12-05T10:52:49.830923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3:  data re-uploading QNN circuit and QNode \n\n# Hyperparameters \nN_LAYERS = 4 \n\ndef qnn_layer(features, weights):\n    \"\"\"One data re-uploading layer: encode features, entangle, then apply trainable rotations.\"\"\"\n    # Encode features (same features on each layer â†’ data re-uploading)\n    for i in range(N_QUBITS):\n        qml.RY(features[i], wires=i)\n        qml.RZ(features[i], wires=i)\n    # Entangling ring\n    for i in range(N_QUBITS - 1):\n        qml.CNOT(wires=[i, i + 1])\n    qml.CNOT(wires=[N_QUBITS - 1, 0])\n    # Trainable single-qubit rotations\n    for i in range(N_QUBITS):\n        qml.RY(weights[i, 0], wires=i)\n        qml.RZ(weights[i, 1], wires=i)\n        qml.RX(weights[i, 2], wires=i)\n\n@qml.qnode(q_device, interface=\"torch\", diff_method=\"adjoint\")\ndef qnode_qnn(features, weights):\n    \"\"\"Full QNN: stack several data re-uploading layers and measure PauliZ on wire 0.\"\"\"\n    # Expect features to be length N_QUBITS here (weâ€™ll map from raw features later)\n    for layer in range(N_LAYERS):\n        qnn_layer(features, weights[layer])\n    return qml.expval(qml.PauliZ(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:52:52.349688Z","iopub.execute_input":"2025-12-05T10:52:52.350336Z","iopub.status.idle":"2025-12-05T10:52:52.356615Z","shell.execute_reply.started":"2025-12-05T10:52:52.350306Z","shell.execute_reply":"2025-12-05T10:52:52.355857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 4: wrap the QNode into a PyTorch nn.Module with a classical feature map\n\nclass QuantumPassSuccessQNN(nn.Module):\n    def __init__(self, n_features, n_qubits=N_QUBITS, n_layers=N_LAYERS):\n        super().__init__()\n        assert n_qubits == N_QUBITS, \"For now, n_qubits must match N_QUBITS used by the QNode.\"\n        self.n_features = n_features\n        self.n_qubits = n_qubits\n        self.n_layers = n_layers\n\n        # Linear feature map: arbitrary feature dimension â†’ N_QUBITS (then scaled to angles)\n        self.feature_map = nn.Linear(n_features, n_qubits)\n\n        # Trainable quantum circuit parameters: (layers, qubits, 3 rotations per qubit)\n        init_scale = 0.01\n        self.q_params = nn.Parameter(\n            init_scale * torch.randn(n_layers, n_qubits, 3, dtype=torch.float32)\n        )\n\n    def forward(self, x):\n        # x: [batch_size, n_features]\n        x = x.to(torch_device)\n        x = self.feature_map(x)                     # [batch_size, n_qubits]\n        x = torch.tanh(x) * math.pi                 # keep angles in [-Ï€, Ï€]\n\n        # Evaluate QNode sample-by-sample (simple but clear; OK for moderate dataset sizes)\n        outputs = []\n        for sample in x:\n            out = qnode_qnn(sample, self.q_params)  # scalar expectation value in [-1, 1]\n            outputs.append(out)\n\n        outputs = torch.stack(outputs)              # [batch_size]\n        # Map [-1, 1] â†’ logits for binary classification (weâ€™ll apply BCEWithLogitsLoss later)\n        logits = outputs.unsqueeze(1)               # [batch_size, 1]\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:52:54.879754Z","iopub.execute_input":"2025-12-05T10:52:54.880421Z","iopub.status.idle":"2025-12-05T10:52:54.886184Z","shell.execute_reply.started":"2025-12-05T10:52:54.880398Z","shell.execute_reply":"2025-12-05T10:52:54.885532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 5: auto-detect StatsBomb dataset folder and list all events directories\n\nimport os, json, glob, random\nimport pandas as pd\n\nBASE_INPUT_DIR = \"/kaggle/input\"\n\nprint(\"Datasets under /kaggle/input:\")\ndatasets = os.listdir(BASE_INPUT_DIR)\nfor d in datasets:\n    print(\" -\", d)\n\n# Try to auto-pick a StatsBomb-like folder\nsb_candidates = [d for d in datasets if \"statsbomb\" in d.lower()]\nif not sb_candidates:\n    raise FileNotFoundError(\n        \"No dataset with 'statsbomb' in its name found under /kaggle/input.\\n\"\n        \"â†’ In the right sidebar, click '+ Add data' and add the 'StatsBomb Football Data' dataset.\"\n    )\n\nSB_DIR_NAME = sb_candidates[0]\nSB_DIR = os.path.join(BASE_INPUT_DIR, SB_DIR_NAME)\nprint(f\"\\nUsing StatsBomb dataset folder: {SB_DIR}\")\n\n# Find all directories named 'events' inside this dataset\nevent_dirs = []\nfor root, dirs, files in os.walk(SB_DIR):\n    if os.path.basename(root).lower() == \"events\":\n        event_dirs.append(root)\n\nif not event_dirs:\n    raise FileNotFoundError(\n        f\"No 'events' directories found inside {SB_DIR}.\\n\"\n        \"Check the dataset structure or confirm that it really contains StatsBomb event JSONs.\"\n    )\n\nprint(\"\\nFound events directories:\")\nfor ed in event_dirs:\n    print(\" -\", ed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:53:00.371377Z","iopub.execute_input":"2025-12-05T10:53:00.371901Z","iopub.status.idle":"2025-12-05T10:53:06.461756Z","shell.execute_reply.started":"2025-12-05T10:53:00.371878Z","shell.execute_reply":"2025-12-05T10:53:06.461029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 6 (FULLY FIXED): extract pass events safely + reservoir sampling\n\nimport numpy as np\n\ndef safe_get(d, key, default=None):\n    \"\"\"Returns d[key] if d is a dict, otherwise returns default.\"\"\"\n    if isinstance(d, dict):\n        return d.get(key, default)\n    return default\n\ndef safe_list(v, length=2):\n    \"\"\"Ensures v is a list of given length (pads with None).\"\"\"\n    if isinstance(v, list):\n        out = v + [None]*length\n        return out[:length]\n    return [None]*length\n\nMAX_PASSES = 200_000\n\npass_rows = []\ntotal_pass_seen = 0\n\nfor ed in event_dirs:\n    event_files = glob.glob(os.path.join(ed, \"*.json\"))\n    print(f\"Processing {len(event_files)} event files from: {ed}\")\n\n    for ef in event_files:\n        with open(ef, \"r\") as f:\n            events = json.load(f)\n\n        for e in events:\n\n            # Filter only Pass events\n            if safe_get(e, \"type\", {}).get(\"name\") != \"Pass\":\n                continue\n\n            total_pass_seen += 1\n\n            # SAFE extraction of all fields\n            team_name   = safe_get(safe_get(e, \"team\"),   \"name\")\n            player_name = safe_get(safe_get(e, \"player\"), \"name\")\n\n            minute = safe_get(e, \"minute\")\n            second = safe_get(e, \"second\")\n            period = safe_get(safe_get(e, \"period\"), \"id\")\n\n            loc = safe_list(safe_get(e, \"location\", [None, None]), 2)\n            p   = safe_get(e, \"pass\", {}) or {}\n\n            end_loc = safe_list(p.get(\"end_location\", [None, None]), 2)\n\n            height_info  = safe_get(p, \"height\", {})\n            outcome_info = safe_get(p, \"outcome\", {})\n\n            row = {\n                \"team\": team_name,\n                \"player\": player_name,\n                \"minute\": minute,\n                \"second\": second,\n                \"period\": period,\n                \"start_x\": loc[0],\n                \"start_y\": loc[1],\n                \"end_x\": end_loc[0],\n                \"end_y\": end_loc[1],\n                \"length\": safe_get(p, \"length\"),\n                \"angle\": safe_get(p, \"angle\"),\n                \"height\": safe_get(height_info, \"name\"),\n                \"under_pressure\": bool(e.get(\"under_pressure\", False)),\n                \"outcome\": safe_get(outcome_info, \"name\"),  # None or 'Complete' = success\n            }\n\n            # Reservoir sampling\n            if len(pass_rows) < MAX_PASSES:\n                pass_rows.append(row)\n            else:\n                j = np.random.randint(0, total_pass_seen)\n                if j < MAX_PASSES:\n                    pass_rows[j] = row\n\nprint(\"\\nTotal pass events seen:\", total_pass_seen)\nprint(\"Total pass events kept:\", len(pass_rows))\n\ndf_passes = pd.DataFrame(pass_rows)\nprint(\"\\ndf_passes shape:\", df_passes.shape)\nprint(\"Outcome counts:\", df_passes['outcome'].value_counts(dropna=False).head(10))\ndf_passes.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T10:53:19.189424Z","iopub.execute_input":"2025-12-05T10:53:19.190011Z","iopub.status.idle":"2025-12-05T10:59:15.709410Z","shell.execute_reply.started":"2025-12-05T10:53:19.189980Z","shell.execute_reply":"2025-12-05T10:59:15.708600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 7: build numeric feature matrix + success labels from df_passes\n\nfrom math import sqrt\n\n# 1) Define success label: None or 'Complete' = success (1), everything else = failure (0)\noutcome_str = df_passes[\"outcome\"].astype(str).str.lower()\nsuccess_mask = df_passes[\"outcome\"].isna() | (outcome_str == \"complete\")\ndf_passes[\"label_success\"] = success_mask.astype(int)\n\nprint(\"Pass success rate:\", df_passes[\"label_success\"].mean())\n\n# 2) Encode pass height as numeric (Ground/Low/High â†’ 0/1/2; others â†’ -1)\nheight_map = {\"ground pass\": 0, \"low pass\": 1, \"high pass\": 2}\nheight_code = df_passes[\"height\"].astype(str).str.lower().map(height_map).fillna(-1)\n\n# 3) Basic geometry\nstart_x = df_passes[\"start_x\"].astype(float)\nstart_y = df_passes[\"start_y\"].astype(float)\nend_x   = df_passes[\"end_x\"].astype(float)\nend_y   = df_passes[\"end_y\"].astype(float)\nlength  = df_passes[\"length\"].astype(float)\nangle   = df_passes[\"angle\"].astype(float)\n\n# 4) Derived geometry features\ndx = end_x - start_x\ndy = end_y - start_y\n\n# Assume StatsBomb pitch: 120 x 80, goal at (120, 40)\nGOAL_X, GOAL_Y = 120.0, 40.0\ndist_start_to_goal = ((GOAL_X - start_x)**2 + (GOAL_Y - start_y)**2) ** 0.5\ndist_end_to_goal   = ((GOAL_X - end_x)**2 + (GOAL_Y - end_y)**2) ** 0.5\nforward_progress   = end_x - start_x  # positive if pass moves ball towards opponent goal\n\n# 5) Context features\nminute = df_passes[\"minute\"].astype(float)\nsecond = df_passes[\"second\"].astype(float)\nperiod = df_passes[\"period\"].astype(float)\nunder_pressure = df_passes[\"under_pressure\"].astype(bool).astype(float)\n\n# 6) Stack all numeric features into a matrix\nfeature_cols = [\n    start_x, start_y,\n    end_x, end_y,\n    length, angle,\n    dx, dy,\n    dist_start_to_goal, dist_end_to_goal,\n    forward_progress,\n    height_code.astype(float),\n    under_pressure,\n    minute, second, period,\n]\n\nX_np = np.vstack([col.values for col in feature_cols]).T  # shape [n_samples, n_features]\ny_np = df_passes[\"label_success\"].astype(np.float32).values\n\nprint(\"Feature matrix shape:\", X_np.shape)\nprint(\"Label vector shape:\", y_np.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:00:27.344587Z","iopub.execute_input":"2025-12-05T11:00:27.345316Z","iopub.status.idle":"2025-12-05T11:00:27.591523Z","shell.execute_reply.started":"2025-12-05T11:00:27.345290Z","shell.execute_reply":"2025-12-05T11:00:27.590702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 8: scale features, split data, build tensors, and initialize QNN model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# 1) Train/val/test split (on raw features to avoid leakage)\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X_np, y_np, test_size=0.3, random_state=SEED, stratify=y_np\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n)\n\nprint(\"Raw splits shapes:\",\n      \"train\", X_train.shape, \"val\", X_val.shape, \"test\", X_test.shape)\n\n# 2) Fit scaler on TRAIN ONLY, then transform all splits\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled   = scaler.transform(X_val)\nX_test_scaled  = scaler.transform(X_test)\n\nprint(\"Train feature means (approx):\", X_train_scaled.mean(axis=0)[:5])\nprint(\"Train feature stds (approx):\",  X_train_scaled.std(axis=0)[:5])\n\n# 3) Convert to torch tensors\nX_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(torch_device)\ny_train_t = torch.tensor(y_train,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\nX_val_t = torch.tensor(X_val_scaled, dtype=torch.float32).to(torch_device)\ny_val_t = torch.tensor(y_val,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\nX_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(torch_device)\ny_test_t = torch.tensor(y_test,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\n# 4) Build QNN model and initialize weights\nn_features = X_train_t.shape[1]\nq_model = QuantumPassSuccessQNN(n_features=n_features).to(torch_device)\n\n# Initialize classical and quantum weights\nfor m in q_model.modules():\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\nwith torch.no_grad():\n    q_model.q_params.copy_(0.01 * torch.randn_like(q_model.q_params))\n\nprint(\"QNN model ready with\", n_features, \"input features on\", torch_device)\nq_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:00:30.464858Z","iopub.execute_input":"2025-12-05T11:00:30.465729Z","iopub.status.idle":"2025-12-05T11:00:31.091095Z","shell.execute_reply.started":"2025-12-05T11:00:30.465701Z","shell.execute_reply":"2025-12-05T11:00:31.090513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 9: create a balanced 50/50 subset with at most 10k samples\n\nMAX_TOTAL_SAMPLES = 20_000  # cap total samples used for QNN training\n\n# y_np and X_np come from CELL 7 (full 200k feature matrix + labels)\nassert X_np.shape[0] == y_np.shape[0]\n\nidx_success = np.where(y_np == 1)[0]\nidx_failure = np.where(y_np == 0)[0]\n\nn_success = len(idx_success)\nn_failure = len(idx_failure)\nprint(\"Original counts â†’ success:\", n_success, \"failure:\", n_failure)\n\n# how many per class we can keep\nmax_per_class = MAX_TOTAL_SAMPLES // 2\nn_per_class = min(n_success, n_failure, max_per_class)\nprint(\"Using per-class samples:\", n_per_class, \"(total =\", 2 * n_per_class, \")\")\n\nrng = np.random.default_rng(SEED)\nidx_success_sub = rng.choice(idx_success, size=n_per_class, replace=False)\nidx_failure_sub = rng.choice(idx_failure, size=n_per_class, replace=False)\n\nidx_balanced = np.concatenate([idx_success_sub, idx_failure_sub])\nrng.shuffle(idx_balanced)\n\nX_small = X_np[idx_balanced]\ny_small = y_np[idx_balanced].astype(np.float32)\n\nprint(\"Balanced subset shapes:\", X_small.shape, y_small.shape)\nprint(\"Balanced success rate (should be ~0.5):\", y_small.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:00:33.649196Z","iopub.execute_input":"2025-12-05T11:00:33.649877Z","iopub.status.idle":"2025-12-05T11:00:33.663268Z","shell.execute_reply.started":"2025-12-05T11:00:33.649839Z","shell.execute_reply":"2025-12-05T11:00:33.662581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 10: scale balanced features, split, build tensors, and initialize QNN\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# 1) train/val/test split on balanced subset\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X_small, y_small, test_size=0.3, random_state=SEED, stratify=y_small\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n)\n\nprint(\"Balanced splits:\",\n      \"train\", X_train.shape, \"val\", X_val.shape, \"test\", X_test.shape)\n\n# 2) fit scaler on TRAIN only, then transform all splits\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled   = scaler.transform(X_val)\nX_test_scaled  = scaler.transform(X_test)\n\nprint(\"Train means (â‰ˆ0):\", X_train_scaled.mean(axis=0)[:5])\nprint(\"Train stds  (â‰ˆ1):\", X_train_scaled.std(axis=0)[:5])\n\n# 3) convert to torch tensors\nX_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(torch_device)\ny_train_t = torch.tensor(y_train,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\nX_val_t = torch.tensor(X_val_scaled, dtype=torch.float32).to(torch_device)\ny_val_t = torch.tensor(y_val,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\nX_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(torch_device)\ny_test_t = torch.tensor(y_test,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\n# 4) build QNN model and initialize weights fresh\nn_features = X_train_t.shape[1]\nq_model = QuantumPassSuccessQNN(n_features=n_features).to(torch_device)\n\nfor m in q_model.modules():\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n\nwith torch.no_grad():\n    q_model.q_params.copy_(0.01 * torch.randn_like(q_model.q_params))\n\nprint(\"QNN model ready with\", n_features, \"features on\", torch_device)\nq_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:00:36.641281Z","iopub.execute_input":"2025-12-05T11:00:36.642013Z","iopub.status.idle":"2025-12-05T11:00:36.701412Z","shell.execute_reply.started":"2025-12-05T11:00:36.641973Z","shell.execute_reply":"2025-12-05T11:00:36.700602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 11: sanitize scaled arrays and create PyTorch DataLoaders\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Clean NaNs and infs in scaled arrays\nX_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)\nX_val_scaled   = np.nan_to_num(X_val_scaled,   nan=0.0, posinf=0.0, neginf=0.0)\nX_test_scaled  = np.nan_to_num(X_test_scaled,  nan=0.0, posinf=0.0, neginf=0.0)\n\n# Rebuild tensors (overwrite)\nX_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(torch_device)\ny_train_t = torch.tensor(y_train,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\nX_val_t = torch.tensor(X_val_scaled, dtype=torch.float32).to(torch_device)\ny_val_t = torch.tensor(y_val,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\nX_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(torch_device)\ny_test_t = torch.tensor(y_test,        dtype=torch.float32).unsqueeze(1).to(torch_device)\n\n# DataLoaders\nBATCH_SIZE = 32\n\ntrain_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=BATCH_SIZE, shuffle=False)\ntest_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=BATCH_SIZE, shuffle=False)\n\nlen(train_loader), len(val_loader), len(test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:00:42.335186Z","iopub.execute_input":"2025-12-05T11:00:42.335447Z","iopub.status.idle":"2025-12-05T11:00:42.352036Z","shell.execute_reply.started":"2025-12-05T11:00:42.335427Z","shell.execute_reply":"2025-12-05T11:00:42.351135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 12 (UPDATED): training loop with tqdm progress bar, best checkpoint, and loss history\n\nfrom tqdm.notebook import tqdm\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(q_model.parameters(), lr=0.01)\n\nEPOCHS = 15\nBEST_MODEL_PATH = \"best_qnn_pass_success.pt\"\n\ntrain_iter_losses = []   # per-iteration training losses (for plotting)\nval_epoch_losses = []    # per-epoch validation losses\nval_epoch_accs = []      # per-epoch validation accuracies\n\nbest_val_acc = 0.0\n\ndef eval_on_loader(model, loader):\n    model.eval()\n    total_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for xb, yb in loader:\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            total_loss += loss.item() * xb.size(0)\n            preds = (torch.sigmoid(logits) >= 0.5).float()\n            total_correct += (preds == yb).sum().item()\n            total_samples += xb.size(0)\n    return total_loss / total_samples, total_correct / total_samples\n\nfor epoch in range(1, EPOCHS + 1):\n    q_model.train()\n    running_loss = 0.0\n\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n    for xb, yb in pbar:\n        optimizer.zero_grad()\n        logits = q_model(xb)\n        loss = criterion(logits, yb)\n        loss.backward()\n        optimizer.step()\n\n        batch_loss = loss.item()\n        running_loss += batch_loss * xb.size(0)\n        train_iter_losses.append(batch_loss)\n\n        pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n\n    train_loss = running_loss / len(train_loader.dataset)\n    val_loss, val_acc = eval_on_loader(q_model, val_loader)\n\n    val_epoch_losses.append(val_loss)\n    val_epoch_accs.append(val_acc)\n\n    # Save best model by validation accuracy\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(q_model.state_dict(), BEST_MODEL_PATH)\n\n    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | \"\n          f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | best_val_acc={best_val_acc:.4f}\")\n\n# Load best weights before final test evaluation\nprint(f\"\\nLoading best model from: {BEST_MODEL_PATH}\")\nq_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=torch_device))\n\ntest_loss, test_acc = eval_on_loader(q_model, test_loader)\nprint(f\"Final TEST â†’ loss={test_loss:.4f} | acc={test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T22:39:49.513523Z","iopub.execute_input":"2025-12-04T22:39:49.513793Z","iopub.status.idle":"2025-12-05T00:56:06.304936Z","shell.execute_reply.started":"2025-12-04T22:39:49.513771Z","shell.execute_reply":"2025-12-05T00:56:06.304211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 13: plot training (per-iteration) loss and validation (per-epoch) loss\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Training loss vs iterations\nplt.figure(figsize=(8, 4))\nplt.plot(range(len(train_iter_losses)), train_iter_losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Train loss (BCE)\")\nplt.title(\"Training loss vs iterations\")\nplt.grid(True)\nplt.show()\n\n# Validation loss vs epochs\nplt.figure(figsize=(6, 4))\nplt.plot(range(1, len(val_epoch_losses) + 1), val_epoch_losses, marker=\"o\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation loss (BCE)\")\nplt.title(\"Validation loss vs epochs\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T00:56:06.306954Z","iopub.execute_input":"2025-12-05T00:56:06.307439Z","iopub.status.idle":"2025-12-05T00:56:06.778852Z","shell.execute_reply.started":"2025-12-05T00:56:06.307419Z","shell.execute_reply":"2025-12-05T00:56:06.778290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 14 (FIXED): correct path to the uploaded model file\n\nMODEL_DIR = \"/kaggle/input/best-qnn-pass-success-pt\"\nMODEL_PATH = os.path.join(MODEL_DIR, \"best_qnn_pass_success.pt\")\n\nprint(\"Model path:\", MODEL_PATH)\n\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:51:30.648200Z","iopub.execute_input":"2025-12-05T11:51:30.648886Z","iopub.status.idle":"2025-12-05T11:51:30.663411Z","shell.execute_reply.started":"2025-12-05T11:51:30.648842Z","shell.execute_reply":"2025-12-05T11:51:30.662898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 15: load pretrained QNN weights and evaluate on the test set\n\nq_model.load_state_dict(torch.load(MODEL_PATH, map_location=torch_device))\nq_model.to(torch_device)\nq_model.eval()\n\nprint(\"Loaded model from:\", MODEL_PATH)\n\n# If eval_on_loader isn't available:\ncriterion = nn.BCEWithLogitsLoss()\n\ndef eval_on_loader(model, loader):\n    model.eval()\n    total_loss, total_correct, total_samples = 0.0, 0, 0\n    with torch.no_grad():\n        for xb, yb in loader:\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            preds = (torch.sigmoid(logits) >= 0.5).float()\n            total_loss += loss.item() * xb.size(0)\n            total_correct += (preds == yb).sum().item()\n            total_samples += xb.size(0)\n    return total_loss / total_samples, total_correct / total_samples\n\ntest_loss, test_acc = eval_on_loader(q_model, test_loader)\nprint(f\"Restored model â†’ loss={test_loss:.4f} | acc={test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:51:37.063431Z","iopub.execute_input":"2025-12-05T11:51:37.064101Z","iopub.status.idle":"2025-12-05T11:52:30.375985Z","shell.execute_reply.started":"2025-12-05T11:51:37.064080Z","shell.execute_reply":"2025-12-05T11:52:30.375250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 16: feature names and helper for QNN probabilities\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Must match the order we used when building X_np in CELL 7\nfeature_names = [\n    \"start_x\", \"start_y\",\n    \"end_x\", \"end_y\",\n    \"length\", \"angle\",\n    \"dx\", \"dy\",\n    \"dist_start_to_goal\", \"dist_end_to_goal\",\n    \"forward_progress\",\n    \"height_code\",\n    \"under_pressure\",\n    \"minute\", \"second\", \"period\",\n]\n\nassert len(feature_names) == X_train.shape[1]\n\ndef qnn_probs_from_raw(X_raw):\n    \"\"\"\n    X_raw: numpy array [n_samples, n_features] in ORIGINAL feature space (not scaled).\n    Returns: numpy array of probabilities P(success) via sigmoid(q_model(x)).\n    \"\"\"\n    X_scaled = scaler.transform(X_raw)                     # scale with train-fitted scaler\n    X_t = torch.tensor(X_scaled, dtype=torch.float32).to(torch_device)\n    with torch.no_grad():\n        logits = q_model(X_t)\n        probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n    return probs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:52:30.377154Z","iopub.execute_input":"2025-12-05T11:52:30.377371Z","iopub.status.idle":"2025-12-05T11:52:30.382881Z","shell.execute_reply.started":"2025-12-05T11:52:30.377355Z","shell.execute_reply":"2025-12-05T11:52:30.382001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 17: decision boundary over (length, forward_progress)\n\n# pick the two features we want to visualize\nf1_name = \"length\"\nf2_name = \"forward_progress\"\n\nf1_idx = feature_names.index(f1_name)\nf2_idx = feature_names.index(f2_name)\n\n# use original (unscaled) validation subset for ranges\nX_val_orig = X_val  # from CELL 10, before scaling\n\nf1_vals = X_val_orig[:, f1_idx]\nf2_vals = X_val_orig[:, f2_idx]\n\nf1_min, f1_max = np.percentile(f1_vals, [2, 98])\nf2_min, f2_max = np.percentile(f2_vals, [2, 98])\n\n# grid in original feature space\ngrid_size = 60\nf1_grid = np.linspace(f1_min, f1_max, grid_size)\nf2_grid = np.linspace(f2_min, f2_max, grid_size)\n\n# base point = mean of train features (original space)\nbase_vec = X_train.mean(axis=0)\n\ngrid_points = []\nfor v1 in f1_grid:\n    for v2 in f2_grid:\n        x = base_vec.copy()\n        x[f1_idx] = v1\n        x[f2_idx] = v2\n        grid_points.append(x)\n\ngrid_points = np.stack(grid_points, axis=0)  # [grid_size^2, n_features]\n\n# get QNN probabilities on the grid\nprobs = qnn_probs_from_raw(grid_points)\nprobs_grid = probs.reshape(grid_size, grid_size)\n\n# plot contour + real points (validation set)\nplt.figure(figsize=(7, 5))\ncs = plt.contourf(f1_grid, f2_grid, probs_grid.T, levels=20, alpha=0.7)\nplt.colorbar(cs, label=\"P(success)\")\n\n# overlay validation samples (downsample for clarity)\nidx_sample = np.random.choice(len(X_val_orig), size=min(1000, len(X_val_orig)), replace=False)\nplt.scatter(\n    X_val_orig[idx_sample, f1_idx],\n    X_val_orig[idx_sample, f2_idx],\n    c=y_val[idx_sample],\n    edgecolors=\"k\",\n    linewidths=0.3,\n    alpha=0.7,\n)\nplt.xlabel(f1_name)\nplt.ylabel(f2_name)\nplt.title(\"QNN decision surface: pass length vs forward progress\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:52:30.383781Z","iopub.execute_input":"2025-12-05T11:52:30.383989Z","iopub.status.idle":"2025-12-05T11:53:34.450065Z","shell.execute_reply.started":"2025-12-05T11:52:30.383972Z","shell.execute_reply":"2025-12-05T11:53:34.449288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 18: permutation feature importance based on validation accuracy\n\nfrom copy import deepcopy\n\n# baseline accuracy on current val_loader\nbaseline_val_loss, baseline_val_acc = eval_on_loader(q_model, val_loader)\nprint(f\"Baseline val_acc = {baseline_val_acc:.4f}\")\n\nimportances = []\n\n# work with numpy copy of scaled validation features\nX_val_scaled_np = X_val_scaled.copy()\n\nfor j, fname in enumerate(feature_names):\n    X_perm = X_val_scaled_np.copy()\n    # permute column j\n    perm_indices = np.random.permutation(X_perm.shape[0])\n    X_perm[:, j] = X_perm[perm_indices, j]\n\n    # build a temporary DataLoader\n    X_perm_t = torch.tensor(X_perm, dtype=torch.float32).to(torch_device)\n    y_val_t_local = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(torch_device)\n    perm_loader = DataLoader(TensorDataset(X_perm_t, y_val_t_local), batch_size=32, shuffle=False)\n\n    _, perm_acc = eval_on_loader(q_model, perm_loader)\n    importance = baseline_val_acc - perm_acc\n    importances.append(importance)\n    print(f\"{fname:20s} | acc={perm_acc:.4f} | drop={importance:.4f}\")\n\n# bar plot of importance\nplt.figure(figsize=(10, 4))\norder = np.argsort(importances)[::-1]\nplt.bar(range(len(feature_names)), np.array(importances)[order])\nplt.xticks(range(len(feature_names)), np.array(feature_names)[order], rotation=60, ha=\"right\")\nplt.ylabel(\"Accuracy drop when permuted\")\nplt.title(\"Permutation feature importance (validation set)\")\nplt.tight_layout()\nplt.grid(True, axis=\"y\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:53:34.451157Z","iopub.execute_input":"2025-12-05T11:53:34.451381Z","iopub.status.idle":"2025-12-05T12:08:41.156334Z","shell.execute_reply.started":"2025-12-05T11:53:34.451357Z","shell.execute_reply":"2025-12-05T12:08:41.155588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 21: install gradio and define QNN prediction helper for the mini app\n\n!pip install -q gradio\n\nimport gradio as gr\n\nGOAL_X, GOAL_Y = 120.0, 40.0  # same as before\n\n# map height string to numeric code, consistent with training\nheight_map = {\"Ground Pass\": 0, \"Low Pass\": 1, \"High Pass\": 2}\n\ndef predict_pass_success(start_x, start_y,\n                         end_x, end_y,\n                         height_str,\n                         under_pressure,\n                         minute, second, period):\n    \"\"\"\n    Build the 16D feature vector from UI inputs, scale, and run QNN to get P(success).\n    \"\"\"\n\n    # 1) geometry\n    dx = end_x - start_x\n    dy = end_y - start_y\n    length = (dx**2 + dy**2) ** 0.5\n    # avoid divide-by-zero; if length=0, set angle=0\n    if length > 0:\n        angle = np.arctan2(dy, dx)\n    else:\n        angle = 0.0\n\n    dist_start_to_goal = ((GOAL_X - start_x)**2 + (GOAL_Y - start_y)**2) ** 0.5\n    dist_end_to_goal   = ((GOAL_X - end_x)**2   + (GOAL_Y - end_y)**2) ** 0.5\n    forward_progress   = end_x - start_x\n\n    # 2) height + context\n    height_code = height_map.get(height_str, -1)\n    up_flag = 1.0 if under_pressure else 0.0\n\n    # 3) pack into the same feature order used for training (original space)\n    x_raw = np.array([\n        start_x, start_y,\n        end_x,   end_y,\n        length,  angle,\n        dx, dy,\n        dist_start_to_goal, dist_end_to_goal,\n        forward_progress,\n        float(height_code),\n        up_flag,\n        float(minute), float(second), float(period),\n    ], dtype=np.float32).reshape(1, -1)\n\n    # 4) scale and run QNN (using existing scaler & q_model)\n    x_scaled = scaler.transform(x_raw)\n    x_scaled = np.nan_to_num(x_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n\n    x_t = torch.tensor(x_scaled, dtype=torch.float32).to(torch_device)\n    q_model.eval()\n    with torch.no_grad():\n        logits = q_model(x_t)\n        prob = torch.sigmoid(logits).cpu().numpy().reshape(-1)[0]\n\n    # 5) human-readable message\n    label = \"Likely SUCCESS âœ…\" if prob >= 0.5 else \"Likely FAILURE âŒ\"\n    return {\n        \"Probability of success\": float(prob),\n        \"Prediction\": label,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:17:48.103187Z","iopub.execute_input":"2025-12-05T12:17:48.103907Z","iopub.status.idle":"2025-12-05T12:17:57.960988Z","shell.execute_reply.started":"2025-12-05T12:17:48.103881Z","shell.execute_reply":"2025-12-05T12:17:57.960309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 22 (FIXED): Gradio UI for interactive QNN pass-success testing\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## Quantum Pass Success Predictor âš½ðŸ§ \")\n    gr.Markdown(\n        \"Adjust the pass parameters below (StatsBomb 120Ã—80 pitch) and let the QNN \"\n        \"estimate the probability that the pass will be completed.\"\n    )\n\n    with gr.Row():\n        with gr.Column():\n            start_x = gr.Slider(0, 120, value=60, label=\"Start X (0â€“120)\")\n            start_y = gr.Slider(0, 80,  value=40, label=\"Start Y (0â€“80)\")\n            end_x   = gr.Slider(0, 120, value=80, label=\"End X (0â€“120)\")\n            end_y   = gr.Slider(0, 80,  value=40, label=\"End Y (0â€“80)\")\n\n        with gr.Column():\n            height_str = gr.Radio(\n                choices=[\"Ground Pass\", \"Low Pass\", \"High Pass\"],\n                value=\"Ground Pass\",\n                label=\"Pass height\"\n            )\n            under_pressure = gr.Checkbox(False, label=\"Under pressure?\")\n            minute = gr.Slider(0, 120, value=30, step=1, label=\"Minute\")\n            second = gr.Slider(0, 59, value=0, step=1, label=\"Second\")\n            period = gr.Slider(1, 4, value=1, step=1, label=\"Period (1â€“4)\")\n\n    output = gr.JSON(label=\"QNN output\")\n\n    btn = gr.Button(\"Predict pass success\")\n    btn.click(\n        predict_pass_success,\n        inputs=[start_x, start_y, end_x, end_y,\n                height_str, under_pressure,\n                minute, second, period],\n        outputs=output,\n    )\n\n# IMPORTANT for Kaggle: use share=True and open the printed https://...gradio.live link\ndemo.launch(share=True, inline=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:20:58.529696Z","iopub.execute_input":"2025-12-05T12:20:58.530201Z","iopub.status.idle":"2025-12-05T12:21:00.561486Z","shell.execute_reply.started":"2025-12-05T12:20:58.530179Z","shell.execute_reply":"2025-12-05T12:21:00.560843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 23: mini test program to inspect real test passes and QNN predictions\n\ndef describe_pass(x_raw, y_true, prob):\n    \"\"\"Pretty-print a single pass in football terms + QNN prediction.\"\"\"\n    d = dict(zip(feature_names, x_raw))\n\n    desc = []\n    desc.append(f\"  Start: ({d['start_x']:.1f}, {d['start_y']:.1f})\")\n    desc.append(f\"  End:   ({d['end_x']:.1f}, {d['end_y']:.1f})\")\n    desc.append(f\"  Length: {d['length']:.1f} m, forward_progress: {d['forward_progress']:.1f} m\")\n    desc.append(f\"  Dist to goal (startâ†’end): {d['dist_start_to_goal']:.1f} â†’ {d['dist_end_to_goal']:.1f} m\")\n    hcode = int(d[\"height_code\"])\n    hname = {0: \"Ground\", 1: \"Low\", 2: \"High\"}.get(hcode, f\"code={hcode}\")\n    desc.append(f\"  Height: {hname}, under_pressure: {bool(round(d['under_pressure']))}\")\n    desc.append(f\"  Time: minute={d['minute']:.0f}, second={d['second']:.0f}, period={d['period']:.0f}\")\n    desc.append(f\"  TRUE label: {'SUCCESS' if y_true==1 else 'FAILURE'}\")\n    desc.append(f\"  QNN P(success): {prob:.3f}  â†’  {'SUCCESS' if prob>=0.5 else 'FAILURE'}\")\n    return \"\\n\".join(desc)\n\ndef test_random_passes(n_samples=5):\n    \"\"\"Sample n real test passes and show what the QNN predicts.\"\"\"\n    # X_test, y_test are ORIGINAL (unscaled) test features/labels\n    idxs = np.random.choice(len(X_test), size=min(n_samples, len(X_test)), replace=False)\n    for i, idx in enumerate(idxs, start=1):\n        x_raw = X_test[idx]\n        y_true = int(y_test[idx])\n\n        # scale & predict with QNN\n        x_scaled = scaler.transform(x_raw.reshape(1, -1))\n        x_scaled = np.nan_to_num(x_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n        x_t = torch.tensor(x_scaled, dtype=torch.float32).to(torch_device)\n        q_model.eval()\n        with torch.no_grad():\n            prob = torch.sigmoid(q_model(x_t)).cpu().numpy().reshape(-1)[0]\n\n        print(f\"=== Sample {i} (test index {idx}) ===\")\n        print(describe_pass(x_raw, y_true, prob))\n        print()\n\n# Run a quick check\ntest_random_passes(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:25:16.255108Z","iopub.execute_input":"2025-12-05T12:25:16.255769Z","iopub.status.idle":"2025-12-05T12:25:16.367988Z","shell.execute_reply.started":"2025-12-05T12:25:16.255744Z","shell.execute_reply":"2025-12-05T12:25:16.367320Z"}},"outputs":[],"execution_count":null}]}